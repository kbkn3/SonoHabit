# 実装計画案: 楽器演奏練習補助アプリケーション

## 1. 前提条件

開発体制: 個人開発 (ご自身 + Cursor等のAI支援活用)
ターゲットプラットフォーム: macOS / iOS (Universal App)
UIフレームワーク: SwiftUI
データ永続化/同期:
SwiftData を採用 (ローカルデータ管理)
CloudKit を利用 (メタデータのiCloud同期)
注意点: SwiftData は iOS 17 / macOS 14 以降が必要です。これより古いOSバージョンをサポートする必要がある場合は、Core Data を検討する必要があります。SwiftData+CloudKitの情報はCore Dataに比べまだ少ない可能性があります。
デザイン: 標準コンポーネント主体で機能実装を優先。
開発期間: 特に定めず、学習と並行して段階的に進める。

## 2. 開発環境セットアップ

最新の Xcode をインストール。
Git でバージョン管理リポジトリを作成 (例: GitHub)。
Xcode で新規プロジェクトを作成:
Template: macOS / iOS -> App
Interface: SwiftUI
Storage: SwiftData
Include Tests にチェック推奨
必要に応じてCursor等の開発支援ツールを設定。

## 3. 実装フェーズ (段階的アプローチ)

### フェーズ 1: データモデル定義と練習メニュー基本UI (MVPコア - Part 1)

目標: 練習メニューと練習項目の基本的な情報を登録・表示・編集・削除できるようにする。
タスク:
@Model を使用して SwiftData のデータモデルを定義する (例: PracticeMenu, PracticeItem)。
PracticeMenu: 名前、作成日など。
PracticeItem: 名前、説明、順番、(将来のためのメトロノーム設定等のプレースホルダ)。
SwiftUI で練習メニューの一覧表示画面を作成 (List)。
練習メニューの追加・削除機能の実装 (@Environment(\.modelContext) を利用)。
練習メニュー選択時に、そのメニューに含まれる練習項目の一覧表示画面へ遷移。
練習項目の追加・編集・削除機能の実装。
基本的なCRUD (作成/読み取り/更新/削除) 操作のテスト。

### フェーズ 2: メトロノーム機能実装と連携 (MVPコア - Part 2)

目標: 独立したメトロノーム機能を実装し、練習項目と連携させる。MVPのコア機能完成。
タスク:
AVFoundation (または必要なら Core Audio) を利用してメトロノームエンジンを実装するクラス/構造体を作成。
指定されたBPMで正確なタイミングでイベントを発生させる。
クリック音ファイルを再生する (AVAudioPlayer 等)。
SwiftUIでメトロノーム用のシンプルなコントロールUIを作成（再生/停止ボタン、BPM表示・設定スライダー/テキストフィールド）。
SwiftData モデル (PracticeItem) にメトロノーム関連の属性を追加 (bpm: Int, timeSignatureNumerator: Int, timeSignatureDenominator: Int, totalBars: Int, repeatCount: Int)。
練習項目の詳細画面でメトロノーム設定を入力できるようにUIを修正。
練習メニュー画面または練習項目詳細画面から、「この設定でメトロノームを開始」するボタンを追加し、メトロノームエンジンと連携させる。
メトロノーム機能とデータ連携のテスト。

### フェーズ 3: メトロノーム機能拡張

目標: メトロノーム機能を要件に合わせて拡張する。
タスク:
BPM自動段階上昇機能の実装 (指定範囲、刻み幅、小節数/繰り返し回数に応じた制御)。
アクセント機能の実装 (拍の頭で異なる音を鳴らすなど)。
クリック音の選択機能の実装 (複数の音声ファイルを用意し、選択できるようにする)。
UIの改善とテスト。

### フェーズ 4: 録音機能実装

目標: 基本的な録音機能を追加し、練習項目と紐付ける。
タスク:
AVFoundation (AVAudioEngine, AVAudioRecorder) を利用して録音機能を実装。
入力ソースの選択機能 (内蔵マイク/接続されている外部インターフェースをリストアップ)。AVAudioSession を使用。
可能な範囲での入力ゲイン調整機能 (システム設定への誘導 or AVAudioEngine での制御)。
MP3形式への変換・保存処理 を実装。別途ライブラリ (例: LAME via C bridge) または AVAssetExportSession (対応形式に注意) が必要になる可能性あり。まずはAAC(.m4a)などOS標準で簡単な形式で実装し、後でMP3変換を追加するのも手。
録音ファイルの再生機能 (AVAudioPlayer)。
SwiftData モデル (PracticeItem) に録音データへの参照（ファイルパスなど）を保存する属性を追加。
練習画面に録音開始/停止ボタンを追加し、録音データを該当の練習項目と紐付けて保存。
録音・再生機能のテスト。

### フェーズ 5: データ同期 (CloudKit)

目標: 練習メニューや設定、録音メタデータをiCloudで同期させる。
タスク:
Apple Developer Program への登録が必要。
Xcode プロジェクト設定で iCloud (CloudKit) を有効化し、コンテナを作成。
SwiftData のデータモデルに CloudKit 同期の設定を追加 (@Model の cloudKitContainerIdentifier 等)。
録音ファイル本体は同期対象外となるよう、モデル設計（ファイルパスのみ保存など）を再確認。
異なるデバイス (Mac/iOSシミュレータ/実機) 間でデータが同期されることをテスト。同期競合の基本的なハンドリングを確認。

### フェーズ 6: 音源再生機能

目標: 外部音源ファイルをインポートし、再生コントロールを可能にする。
タスク:
AVFoundation (AVPlayer) を利用した基本的な音源再生機能の実装。
ファイルインポート機能の実装:
Files App (Document Picker) を利用してローカルやiCloud Drive上のファイルを選択させる。
セキュリティスコープを利用してファイルアクセス権を保持。
(オプション) Google Drive/Dropbox API連携は初期スコープでは複雑なため、Files App経由を推奨。
SwiftData モデルにインポートした音源ファイルの情報（ブックマークデータ等）を保存する属性を追加。
A-Bループ機能の実装 (AVPlayer の再生範囲指定)。
テンポ変更・ピッチ変更機能の実装 (AVAudioEngine と AVAudioUnitTimePitch)。音質劣化に注意。
音源再生と録音の同時再生機能（AVAudioEngine を使うと制御しやすい）。音量バランス調整UIの実装。
テスト。

### フェーズ 7: 付加機能と仕上げ

目標: 残りの要件を実装し、アプリ全体の品質を高める。
タスク:
自己評価機能の実装 (UIとデータ保存)。
テンプレート機能の実装 (事前定義されたメニュー/項目をコピーして利用)。
全体的なUI/UXの見直しと改善 (標準コンポーネントの範囲で)。
エラーハンドリングの強化。
必要に応じて単体テスト・UIテストを追加。
アイコン作成、App Store提出に向けた準備（プライバシーポリシー等）。

## 4. 開発Tips

小さく始める: 各フェーズの中でも、さらに小さなステップに分割して進める。
学習と実践: 初めての技術要素については、公式ドキュメント (Apple Developer) やチュートリアルで学習しながら進める。
AI支援の活用: Cursor 等のAIツールをコード生成、デバッグ、不明点の質問に積極的に活用する。
バージョン管理: Git でこまめにコミットし、ブランチを活用して機能開発を行う。
テスト: 各機能が実装できたら、必ず動作確認・テストを行う。
